{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "123fbbe4-0005-4820-af13-5f7a9c12a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_json(fname):\n",
    "    with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdd6a34-7d0a-4eb0-91c6-53f03df6291e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/lustrep3/scratch/project_465000498/vlad/mimir/mimir_results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/appl/local/ood/prod/soft/base_py/3.10/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd mimir_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de1a37a-b2cf-4f24-8361-f76a163c2677",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = [\n",
    "    \"norec_sentence_nb\",\n",
    "    \"norec_document_nb\",\n",
    "    \"mimir_bias\",\n",
    "    \"noridiom_nb\",\n",
    "    \"noridiom_nn\",\n",
    "    \"ncb\",\n",
    "    \"norbelebele_nb\",\n",
    "    \"nrk_nb\",\n",
    "    \"nrk_nn\",\n",
    "    \"noropenbookqa_nb\",\n",
    "    \"noropenbookqa_nb_use_fact\",\n",
    "    \"noropenbookqa_nn\",\n",
    "    \"noropenbookqa_nn_use_fact\",\n",
    "    \"norcommonsenseqa_nb\",\n",
    "    \"norcommonsenseqa_nn\",\n",
    "    \"nortruthfulqa_mc_nb\",\n",
    "    \"nortruthfulqa_mc_nn\",\n",
    "    \"nortruthfulqa_gen_nb\",\n",
    "    \"norquad_nb\",\n",
    "    \"schibsted_vg_nb\",\n",
    "    \"ask_gec_nb\",\n",
    "    \"norsumm_nb\",\n",
    "    \"norsumm_nn\",\n",
    "    \"tatoeba_eng_nno_nn\",\n",
    "    \"tatoeba_nno_eng_nn\",\n",
    "    \"tatoeba_eng_nob_nb\",\n",
    "    \"tatoeba_nob_eng_nb\",\n",
    "    \"tatoeba_nob_nno_nb\",\n",
    "    \"tatoeba_nno_nob_nn\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db11f023-69e2-4849-a2bf-f7b2bb100583",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {\n",
    "    \"norquad_nb\": {\n",
    "        \"prompt-0\": \"Tittel: {title}\\n\\nTekst: {passage}\\n\\nSpørsmål: {question}\\n\\nSvar:\",\n",
    "        \"prompt-1\": 'Tittel: {title}\\n\\nTekst: {passage}\\n\\nGitt teksten over, hva er svaret på følgende spørsmål? \"{question}\"\\n\\nSvar:',\n",
    "        \"prompt-2\": \"Tittel: {title}\\n\\nTekst: {passage}\\n\\nSpørsmål: {question}\\n\\nSvar:\",\n",
    "        \"prompt-3\": 'Tittel: {title}\\n\\nTekst: {passage}\\n\\nHvordan kan man svare på spørsmålet \"{question}\", gitt teksten over?\\n\\nSvar:',\n",
    "        \"prompt-4\": 'Tittel: {title}\\n\\nTekst:{passage}\\n\\nGitt teksten over, besvar følgende spørsmål: \"{question}\"\\n\\nSvar:',\n",
    "    }\n",
    "}\n",
    "\n",
    "task2metric = {\n",
    "    \"mimir_bias\": [\"pct_stereotype\", \"likelihood_diff\"],\n",
    "    \"ncb\": [\"acc\"],\n",
    "    \"norec_sentence_nb\": [\"acc\", \"f1\"],\n",
    "    \"norec_document_nb\": [\"acc\", \"f1\"],\n",
    "    \"tapaco_no_detection_nb\": [\"acc\"],\n",
    "    \"norbelebele_nb\": [\"acc\"],\n",
    "    \"nrk_nb\": [\"acc\"],\n",
    "    \"nrk_nn\": [\"acc\"],\n",
    "    \"noropenbookqa_nb\": [\"acc\"],\n",
    "    \"noropenbookqa_nn\": [\"acc\"],\n",
    "    \"noropenbookqa_nb_use_fact\": [\"acc\"],\n",
    "    \"noropenbookqa_nn_use_fact\": [\"acc\"],\n",
    "    \"norcommonsenseqa_nn\": [\"acc\"],\n",
    "    \"norcommonsenseqa_nb\": [\"acc\"],\n",
    "    \"nortruthfulqa_mc_nb\": [\"acc\"],\n",
    "    \"nortruthfulqa_mc_nn\": [\"acc\"],\n",
    "    \"norquad_nb\": [\"exact_match\", \"f1\"],\n",
    "    \"noridiom_nb\": [\"em\", \"fscore\"],\n",
    "    \"noridiom_nn\": [\"em\", \"fscore\"],\n",
    "    \"norsumm_nb\": [\n",
    "        \"bleu_max\",\n",
    "        \"bleu_avg\",\n",
    "        \"rougeL_max\",\n",
    "        \"rougeL_avg\",\n",
    "        \"bertscore_f1_max\",\n",
    "        \"bertscore_f1_avg\",\n",
    "    ],\n",
    "    \"norsumm_nn\": [\n",
    "        \"bleu_max\",\n",
    "        \"bleu_avg\",\n",
    "        \"rougeL_max\",\n",
    "        \"rougeL_avg\",\n",
    "        \"bertscore_f1_max\",\n",
    "        \"bertscore_f1_avg\",\n",
    "    ],\n",
    "    \"nortruthfulqa_gen_nb\": [\"bleu_max\", \"rougeL_max\"],\n",
    "    \"schibsted_vg_nb\": [\"bleu\", \"chrf\"],\n",
    "    \"ask_gec_nb\": [\"errant\"],\n",
    "    \"tatoeba_eng_nno_nn\": [\"bleu\", \"chrf\"],\n",
    "    \"tatoeba_nno_eng_nn\": [\"bleu\", \"chrf\"],\n",
    "    \"tatoeba_eng_nob_nb\": [\"bleu\", \"chrf\"],\n",
    "    \"tatoeba_nob_eng_nb\": [\"bleu\", \"chrf\"],\n",
    "    \"tatoeba_nob_nno_nb\": [\"bleu\", \"chrf\"],\n",
    "    \"tatoeba_nno_nob_nn\": [\"bleu\", \"chrf\"],\n",
    "}\n",
    "\n",
    "\n",
    "def pretty_metric(\n",
    "    task,\n",
    "    metric_name,\n",
    "    score,\n",
    "    metric_list=[\n",
    "        \"f1\",\n",
    "        \"acc\",\n",
    "        \"pct_stereotype\",\n",
    "        \"acc_norm\",\n",
    "        \"em\",\n",
    "        \"fscore\",\n",
    "        \"bertscore_f1_avg\",\n",
    "        \"bertscore_f1_max\",\n",
    "    ],\n",
    "):\n",
    "    pretty_metric_name = metric_name.replace(\",none\", \"\")\n",
    "    pretty_metric_score = (\n",
    "        round(score * 100, 3) if pretty_metric_name in metric_list else round(score, 3)\n",
    "    )\n",
    "    if task == \"norquad_nb\":\n",
    "        pretty_metric_score = round(score, 3)\n",
    "    return pretty_metric_name, pretty_metric_score\n",
    "\n",
    "\n",
    "def collect_task_ranking_results(\n",
    "    task,\n",
    "    k=0,\n",
    "    ignore_models=[\"gpt-sw3-6.7b\"],\n",
    "    ignore_metrics=[\n",
    "        \"alias\",\n",
    "        \"bleu_acc\",\n",
    "        \"bleu_diff\",\n",
    "        \"rouge1_acc\",\n",
    "        \"rouge1_diff\",\n",
    "        \"rouge2_max\",\n",
    "        \"rouge2_acc\",\n",
    "        \"rouge2_diff\",\n",
    "        \"rougeL_acc\",\n",
    "        \"rougeL_diff\",\n",
    "    ],\n",
    "    verbose=True,\n",
    "    columns=[\"task\", \"model\", \"k-shot\"],\n",
    "):\n",
    "    res = []\n",
    "    res_fdir = f\"{task}/{k}-shot\"\n",
    "    res_columns = columns.copy()\n",
    "    for model_organization in os.listdir(res_fdir):\n",
    "        model_fdir = os.path.join(res_fdir, model_organization)\n",
    "        for model in os.listdir(model_fdir):\n",
    "            if model in ignore_models:\n",
    "                continue\n",
    "            model_res_fpath = os.path.join(model_fdir, model, \"results.json\")\n",
    "            if verbose:\n",
    "                print(model_res_fpath)\n",
    "            model_res = load_json(model_res_fpath)\n",
    "            model_res_scores = model_res[\"results\"][task]\n",
    "            curr_configuration_res = [task, model, k]\n",
    "            for metric_name, score in model_res_scores.items():\n",
    "                if \"stderr\" in metric_name or metric_name in ignore_metrics:\n",
    "                    continue\n",
    "                pretty_metric_name, pretty_metric_score = pretty_metric(\n",
    "                    task=task, metric_name=metric_name, score=score\n",
    "                )\n",
    "                if pretty_metric_name not in res_columns:\n",
    "                    res_columns.append(pretty_metric_name)\n",
    "                curr_configuration_res.append(pretty_metric_score)\n",
    "            res.append(curr_configuration_res)\n",
    "    return pd.DataFrame(res, columns=res_columns)\n",
    "\n",
    "\n",
    "def collect_task_prompt_results(\n",
    "    task,\n",
    "    k,\n",
    "    ignore_models=[\"gpt-sw3-6.7b\"],\n",
    "    verbose=True,\n",
    "    columns=[\"task\", \"model\", \"prompt\", \"k-shot\"],\n",
    "    prompts=prompts,\n",
    "):\n",
    "    res = []\n",
    "    res_fdir = f\"{task}/{k}-shot\"\n",
    "    res_columns = columns.copy()\n",
    "    for model_organization in os.listdir(res_fdir):\n",
    "        model_fdir = os.path.join(res_fdir, model_organization)\n",
    "        for model in os.listdir(model_fdir):\n",
    "            if model in ignore_models:\n",
    "                continue\n",
    "            model_res_fpath = os.path.join(model_fdir, model, \"results.json\")\n",
    "            if not os.path.exists(model_res_fpath):\n",
    "                continue\n",
    "            if verbose:\n",
    "                print(model_res_fpath)\n",
    "            model_res = load_json(model_res_fpath)\n",
    "            model_res_scores = {\n",
    "                prompt_name: prompt_res\n",
    "                for prompt_name, prompt_res in model_res[\"results\"].items()\n",
    "                if prompt_name != task\n",
    "            }\n",
    "            for configuration_name, configuration_res in model_res_scores.items():\n",
    "                prompt = (\n",
    "                    prompts[task][configuration_name]\n",
    "                    if task in prompts\n",
    "                    else model_res[\"configs\"][configuration_name][\"doc_to_text\"]\n",
    "                )\n",
    "                curr_configuration_res = [task, model, prompt, k]\n",
    "                for metric_name, score in configuration_res.items():\n",
    "                    if \"stderr\" in metric_name or metric_name == \"alias\":\n",
    "                        continue\n",
    "                    pretty_metric_name, pretty_metric_score = pretty_metric(\n",
    "                        task=task, metric_name=metric_name, score=score\n",
    "                    )\n",
    "                    if pretty_metric_name not in res_columns:\n",
    "                        res_columns.append(pretty_metric_name)\n",
    "                    curr_configuration_res.append(pretty_metric_score)\n",
    "                res.append(curr_configuration_res)\n",
    "    return pd.DataFrame(res, columns=res_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca74f9a9-d2d6-49b4-81dc-5ad678651ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot = [\n",
    "    \"norec_sentence_nb\",\n",
    "    \"norec_document_nb\",\n",
    "    \"mimir_bias\",\n",
    "    \"tapaco_no_detection_nb\",\n",
    "    \"norsumm_nb\",\n",
    "    \"norsumm_nn\",\n",
    "    \"noridiom_nb\",\n",
    "    \"noridiom_nn\",\n",
    "    \"ncb\",\n",
    "    \"norbelebele_nb\",\n",
    "    \"nrk_nb\",\n",
    "    \"nrk_nn\",\n",
    "    \"noropenbookqa_nb\",\n",
    "    \"noropenbookqa_nb_use_fact\",\n",
    "    \"noropenbookqa_nn\",\n",
    "    \"noropenbookqa_nn_use_fact\",\n",
    "    \"norcommonsenseqa_nb\",\n",
    "    \"norcommonsenseqa_nn\",\n",
    "    \"nortruthfulqa_mc_nb\",\n",
    "    \"nortruthfulqa_mc_nn\",\n",
    "    \"norquad_nb\",\n",
    "    \"nortruthfulqa_gen_nb\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57601aa6-6592-4aa6-bad2-525145628658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "\n",
    "overall = {\n",
    "    task: (\n",
    "        collect_task_prompt_results(task, k=0, verbose=False)\n",
    "        if task not in [\"mimir_bias\", \"ncb\"]\n",
    "        else collect_task_ranking_results(task, k=0, verbose=False)\n",
    "    )\n",
    "    for task in zero_shot\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e97b65ee-f80b-46ac-87d3-a3597bb42ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ranking_results(tasks, overall=overall, task2metric=task2metric, on=\"model\"):\n",
    "    res = {\n",
    "        task_name: overall[task_name].rename(\n",
    "            columns={col: f\"{task_name} ({col})\" for col in task2metric[task_name]}\n",
    "        )\n",
    "        for task_name in tasks\n",
    "    }\n",
    "    df = reduce(\n",
    "        lambda df_left, df_right: pd.merge(df_left, df_right, on=\"model\"),\n",
    "        list(res.values()),\n",
    "    )\n",
    "    df.rename(columns={\"k-shot_x\": \"k-shot\"}, inplace=True)\n",
    "    df = df[\n",
    "        [\n",
    "            col\n",
    "            for col in df.columns\n",
    "            if not any([name in col for name in [\"task_\", \"shot_\"]])\n",
    "        ]\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "\n",
    "def aggregate_df(df, task, task2metric, select_best):\n",
    "    task_res = []\n",
    "    task_columns = [\"model\"] + task2metric[task]\n",
    "    for model, subset in df.groupby(\"model\"):\n",
    "        model_res = [model]\n",
    "        for metric in task2metric[task]:\n",
    "\n",
    "            agg_res = dict(subset[metric].describe())\n",
    "            if select_best:\n",
    "                model_res.append(round(agg_res[\"max\"], 2))\n",
    "            else:\n",
    "                model_res.append(\n",
    "                    f\"{round(agg_res['mean'], 2)} ± {round(agg_res['std'], 1)} [{round(agg_res['max'], 2)}]\"\n",
    "                )\n",
    "        task_res.append(model_res)\n",
    "    return pd.DataFrame(task_res, columns=task_columns)\n",
    "\n",
    "\n",
    "def merge_task_prompt_results(\n",
    "    tasks, overall=overall, task2metric=task2metric, select_best=True, on=\"model\"\n",
    "):\n",
    "    res = {\n",
    "        task_name: aggregate_df(\n",
    "            overall[task_name], task_name, task2metric, select_best\n",
    "        ).rename(\n",
    "            columns={col: f\"{task_name} ({col})\" for col in task2metric[task_name]}\n",
    "        )\n",
    "        for task_name in tasks\n",
    "    }\n",
    "\n",
    "    df = reduce(\n",
    "        lambda df_left, df_right: pd.merge(df_left, df_right, on=\"model\"),\n",
    "        list(res.values()),\n",
    "    )\n",
    "    df.rename(columns={\"k-shot_x\": \"k-shot\"}, inplace=True)\n",
    "    df = df[\n",
    "        [\n",
    "            col\n",
    "            for col in df.columns\n",
    "            if not any(\n",
    "                [\n",
    "                    name in col\n",
    "                    for name in [\n",
    "                        \"task_\",\n",
    "                        \"shot_\",\n",
    "                        \"sentence_nb (acc\",\n",
    "                        \"document_nb (acc\",\n",
    "                        \"acc_norm\",\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7c807d7-aba3-4bd6-9fd8-fa5d6219e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_order = [\n",
    "    \"mimir-mistral-7b-extended\",\n",
    "    \"mimir-mistral-7b-extended-scratch\",\n",
    "    \"mimir-mistral-7b-base\",\n",
    "    \"mimir-mistral-7b-base-scratch\",\n",
    "    \"mimir-7b-fiction\",\n",
    "    \"mimir-7b-nonfiction\",\n",
    "    \"mimir-7b-factual\",\n",
    "    \"mimir-7b-newspapers\",\n",
    "    \"mimir-7b-books\",\n",
    "    \"mimir-7b-rightholders\",\n",
    "    \"mimir-7b-untranslated-withnewspapers\",\n",
    "    \"mimir-7b-untranslated\",\n",
    "    \"mimir-7b-translated\",\n",
    "]\n",
    "\n",
    "skill = {\n",
    "    \"Sentiment analysis\": [\"norec_sentence_nb\", \"norec_document_nb\"],\n",
    "    \"Fairness & truthfulness\": [\n",
    "        \"mimir_bias\",\n",
    "        \"nortruthfulqa_mc_nb\",\n",
    "        \"nortruthfulqa_mc_nn\",\n",
    "        \"nortruthfulqa_gen_nb\",\n",
    "    ],\n",
    "    \"Reading comprehension\": [\"norbelebele_nb\", \"norquad_nb\"],\n",
    "    \"World knowledge\": [\n",
    "        \"nrk_nb\",\n",
    "        \"nrk_nn\",\n",
    "        \"noropenbookqa_nb\",\n",
    "        \"noropenbookqa_nn\",\n",
    "        \"noropenbookqa_nb_use_fact\",\n",
    "        \"noropenbookqa_nn_use_fact\",\n",
    "    ],\n",
    "    \"Commonsense reasoning\": [\"norcommonsenseqa_nb\", \"norcommonsenseqa_nn\"],\n",
    "    \"Norwegian language: grammar, punctuation, and idioms\": [\n",
    "        \"n,cb\",\n",
    "        \"ask_gec_nb\",\n",
    "        \"noridiom_nb\",\n",
    "        \"noridiom_nn\",\n",
    "    ],\n",
    "    \"Text summarization\": [\"norsumm_nb\", \"norsumm_nn\"],\n",
    "    \"Machine translation\": [\n",
    "        \"tatoeba_eng_nno_nn\",\n",
    "        \"tatoeba_nno_eng_nn\",\n",
    "        \"tatoeba_eng_nob_nb\",\n",
    "        \"tatoeba_nob_eng_nb\",\n",
    "        \"tatoeba_nob_nno_nb\",\n",
    "        \"tatoeba_nno_nob_nn\",\n",
    "    ],\n",
    "    \"Headline generation\": [\"schibsted_vg_nb\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "518c7a27-71c4-448f-95ff-1989c6d4946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "beautify_columns = {\n",
    "    \"model\": \"Model\",\n",
    "    \"norec_sentence_nb (f1)\": \"NoReC\",\n",
    "    \"norec_document_nb (f1)\": \"NoReC\",\n",
    "}\n",
    "\n",
    "\n",
    "def pretty_model(model_name):\n",
    "    model_d = {\n",
    "        \"mimir-mistral-7b-base\": \"@base\",\n",
    "        \"mimir-mistral-7b-extended\": \"@extended\",\n",
    "        \"mimir-7b-fiction\": \"@fiction\",\n",
    "        \"mimir-7b-nonfiction\": \"@nonfiction\",\n",
    "        \"mimir-7b-factual\": \"@factual\",\n",
    "        \"mimir-7b-newspapers\": \"@newspapers\",\n",
    "        \"mimir-7b-books\": \"@books\",\n",
    "        \"mimir-7b-rightholders\": \"@rightholders\",\n",
    "        \"mimir-7b-untranslated-withnewspapers\": \"@untranslatedwithnewspapers\",\n",
    "        \"mimir-7b-untranslated\": \"@untranslated\",\n",
    "        \"mimir-7b-translated\": \"@translated\",\n",
    "        \"mimir-mistral-7b-base-scratch\": \"@basescratch\",\n",
    "        \"mimir-mistral-7b-extended-scratch\": \"@extendedscratch\",\n",
    "    }\n",
    "    # mimir, conf = model_name.replace(\"-mistral-\", \"\").split(\"7b\")\n",
    "    # mimir = mimir.replace(\"mimir\", \"\\textsc{mimir}\")\n",
    "    # pretty_name = f\"{mimir}$_\\text\" + \"{\" + f\"{conf.strip(' -')}\".replace(\"\\text\", \"\\text{\") + \"}$\"\n",
    "    return model_d[model_name]\n",
    "    # return pretty_name\n",
    "\n",
    "\n",
    "def aggregate_by_skill(\n",
    "    task,\n",
    "    model_order=canonical_order,\n",
    "    select_best=True,\n",
    "    skill=skill,\n",
    "    add_baselines=[],\n",
    "    add_k=None,\n",
    "    overall=overall,\n",
    "    task2metric=task2metric,\n",
    "    base_model=\"mimir-mistral-7b-base-scratch\",\n",
    "    target_metric=\"f1\",\n",
    "    beautify_columns=beautify_columns,\n",
    "):\n",
    "    df = merge_task_prompt_results(\n",
    "        [task], select_best=select_best, overall=overall, task2metric=task2metric\n",
    "    )\n",
    "    df = df[df[\"model\"].isin(model_order)]\n",
    "    # print(f\"Task: {task}; Num rows: {df.shape[0]}\")\n",
    "    reference_score = {\n",
    "        task: score.item()\n",
    "        for task, score in dict(df[df[\"model\"] == base_model]).items()\n",
    "        if task != \"model\"\n",
    "    }\n",
    "    if add_baselines:\n",
    "        df = pd.concat([df, pd.DataFrame(add_baselines, columns=df.columns.tolist())])\n",
    "\n",
    "    model_order = model_order.copy() + [\n",
    "        baseline_name for baseline_name, _ in add_baselines\n",
    "    ]\n",
    "    ascending = False if \"mimir_bias\" not in task else True\n",
    "    model_rank = {\n",
    "        model: i + 1\n",
    "        for i, model in enumerate(\n",
    "            df.sort_values(\n",
    "                f\"{task} ({target_metric})\", ascending=ascending\n",
    "            ).model.tolist()\n",
    "        )\n",
    "    }\n",
    "    df[\"Rank\"] = df[\"model\"].apply(lambda x: model_rank[x])\n",
    "    df = df.set_index(\"model\").loc[model_order]\n",
    "    agg, agg_cols = [], [\n",
    "        \"Rank\",\n",
    "        \"Model\",\n",
    "        f\"{task} ({target_metric})\",\n",
    "        f\"delta ({target_metric})\",\n",
    "    ]\n",
    "    for model_name, row in df.iterrows():\n",
    "        row_res = [int(row[\"Rank\"])]\n",
    "        if model_name.startswith(\"mimir\"):\n",
    "            row_res.append(pretty_model(model_name))\n",
    "            for task_name, ref in reference_score.items():\n",
    "                if task_name != f\"{task} ({target_metric})\":\n",
    "                    continue\n",
    "                if model_name == base_model:\n",
    "                    row_res.append(ref)\n",
    "                    row_res.append(\"xmark\")\n",
    "                elif (\n",
    "                    model_name.endswith(\"scratch\")\n",
    "                    or (\"base\" in model_name and model_name != base_model)\n",
    "                    or \"extended\" in model_name\n",
    "                ):\n",
    "                    row_res.append(row[task_name])\n",
    "                    row_res.append(\"xmark\")\n",
    "                else:\n",
    "                    model_conf_score = row[task_name]\n",
    "                    row_res.append(model_conf_score)\n",
    "                    delta = round(model_conf_score - ref, 1)\n",
    "                    if delta > 0:\n",
    "                        row_res.append(f\"+{delta}\")\n",
    "                    else:\n",
    "                        row_res.append(f\"-{delta}\")\n",
    "        else:\n",
    "            row_res.extend([model_name, row[f\"{task} ({target_metric})\"], \"xmark\"])\n",
    "        agg.append(row_res)\n",
    "    agg_df = pd.DataFrame(agg, columns=agg_cols)\n",
    "    if add_k is not None:\n",
    "        agg_df[\"k\"] = add_k\n",
    "        agg_df = agg_df[\n",
    "            [\n",
    "                \"Rank\",\n",
    "                \"Model\",\n",
    "                \"k\",\n",
    "                f\"{task} ({target_metric})\",\n",
    "                f\"delta ({target_metric})\",\n",
    "            ]\n",
    "        ]\n",
    "    agg_df.rename(columns=beautify_columns, inplace=True)\n",
    "    agg_df = agg_df.set_index(\"Rank\")\n",
    "    return agg_df\n",
    "\n",
    "\n",
    "def print_latex_df(df):\n",
    "    print(\n",
    "        df.to_latex()\n",
    "        .replace(\"@\", \"\\\\\")\n",
    "        .replace(\"xmark\", \"\\\\xmark\")  # .replace(\"$delta$\", \"delta\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa922d9d-2303-43e9-8e02-d9e6951ba508",
   "metadata": {},
   "source": [
    "### Single-shot tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7042a800-135f-4fd8-8650-0cccb1218d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"norbelebele_nb\"\n",
    "add_baselines = [\n",
    "    [\"Random\", 25.00],\n",
    "]\n",
    "norbelebele_nb = aggregate_by_skill(\n",
    "    task, add_baselines=add_baselines, target_metric=\"acc\"\n",
    ")\n",
    "norbelebele_nb.to_csv(\"agg_results/qa/norbelebele_nb.tsv\", sep=\"\\t\", index=False)\n",
    "# print_latex_df(norbelebele_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3df762a-c56b-4450-9145-db1bb5b39a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"nrk_nb\"\n",
    "add_baselines = [[\"Random\", 27.91], [\"Constant\", 30.97]]\n",
    "nrk_nb = aggregate_by_skill(task, add_baselines=add_baselines, target_metric=\"acc\")\n",
    "nrk_nb.to_csv(\"agg_results/qa/nrk_nb.tsv\", sep=\"\\t\", index=False)\n",
    "# print_latex_df(nrk_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbc9c581-0169-46ea-a685-3af856006d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"nrk_nn\"\n",
    "add_baselines = [[\"Random\", 26.76], [\"Constant\", 30.45]]\n",
    "nkr_nn = aggregate_by_skill(task, add_baselines=add_baselines, target_metric=\"acc\")\n",
    "nkr_nn.to_csv(\"agg_results/qa/nrk_nn.tsv\", sep=\"\\t\", index=False)\n",
    "# print_latex_df(nrk_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f52bb81-dde5-4a64-8cc1-03a8835d4a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"norcommonsenseqa_nb\"\n",
    "add_baselines = [[\"Random\", 20.00]]\n",
    "norcommonsenseqa_nb = aggregate_by_skill(\n",
    "    task, add_baselines=add_baselines, target_metric=\"acc\"\n",
    ")\n",
    "norcommonsenseqa_nb.to_csv(\n",
    "    \"agg_results/qa/norcommonsenseqa_nb.tsv\", sep=\"\\t\", index=False\n",
    ")\n",
    "# print_latex_df(norcommonsenseqa_nb)\n",
    "\n",
    "task = \"norcommonsenseqa_nn\"\n",
    "add_baselines = [[\"Random\", 20.00]]\n",
    "norcommonsenseqa_nn = aggregate_by_skill(\n",
    "    task, add_baselines=add_baselines, target_metric=\"acc\"\n",
    ")\n",
    "norcommonsenseqa_nn.to_csv(\n",
    "    \"agg_results/qa/norcommonsenseqa_nn.tsv\", sep=\"\\t\", index=False\n",
    ")\n",
    "# print_latex_df(aggregate_by_skill(task, add_baselines=add_baselines, target_metric=\"acc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c4a2aa8-c29b-4fb6-915e-4475125c3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir agg_results/ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69df086b-ed33-472d-bab7-40c2ec07d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ncb\"\n",
    "add_baselines = [\n",
    "    [\"Random\", 50.00],\n",
    "]\n",
    "ncb = aggregate_by_skill(task, add_baselines=add_baselines, target_metric=\"acc\")\n",
    "ncb.to_csv(\"agg_results/ranking/ncb.tsv\", sep=\"\\t\", index=False)\n",
    "# print_latex_df(ncb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b4b0f74-288b-4963-909d-6c738334e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir agg_results/generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ab61f4a-3a9e-4445-84cb-4e5b291089c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"noridiom_nb\"\n",
    "add_baselines = []\n",
    "em = aggregate_by_skill(task, add_baselines=add_baselines, target_metric=\"em\")\n",
    "f1 = aggregate_by_skill(task, add_baselines=add_baselines, target_metric=\"fscore\")\n",
    "em[\"Rank\"] = em.index.tolist()\n",
    "noridiom_nb = (\n",
    "    em[[\"Rank\", \"Model\", \"noridiom_nb (em)\", \"delta (em)\"]]\n",
    "    .merge(f1, on=\"Model\")\n",
    "    .set_index(\"Rank\")\n",
    ")\n",
    "noridiom_nb.to_csv(\"agg_results/generation/noridiom_nb.tsv\", sep=\"\\t\", index=False)\n",
    "# print_latex_df(noridiom_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2764ce44-da73-4855-b40a-d6347b8c3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"noridiom_nn\"\n",
    "add_baselines = []\n",
    "em = aggregate_by_skill(task, add_baselines=add_baselines, target_metric=\"em\")\n",
    "f1 = aggregate_by_skill(task, add_baselines=add_baselines, target_metric=\"fscore\")\n",
    "em[\"Rank\"] = em.index.tolist()\n",
    "noridiom_nn = (\n",
    "    em[[\"Rank\", \"Model\", \"noridiom_nn (em)\", \"delta (em)\"]]\n",
    "    .merge(f1, on=\"Model\")\n",
    "    .set_index(\"Rank\")\n",
    ")\n",
    "noridiom_nn.to_csv(\"agg_results/generation/noridiom_nn.tsv\", sep=\"\\t\", index=False)\n",
    "# print_latex_df(noridiom_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7a926bc-6a6b-4cda-9ce1-c68816a0af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"nortruthfulqa_mc_nb\"\n",
    "add_baselines = [[\"Random\", 27.27]]\n",
    "nortruthfulqa_mc_nb = aggregate_by_skill(\n",
    "    task, add_baselines=add_baselines, target_metric=\"acc\"\n",
    ")\n",
    "nortruthfulqa_mc_nb.to_csv(\n",
    "    \"agg_results/qa/nortruthfulqa_mc_nb.tsv\", sep=\"\\t\", index=False\n",
    ")\n",
    "# print_latex_df(nortruthfulqa_mc_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "388b8fe0-1594-44d1-acad-7a351bfa4457",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"nortruthfulqa_mc_nn\"\n",
    "add_baselines = [[\"Random\", 24.56]]\n",
    "nortruthfulqa_mc_nn = aggregate_by_skill(\n",
    "    task, add_baselines=add_baselines, target_metric=\"acc\"\n",
    ")\n",
    "nortruthfulqa_mc_nn.to_csv(\n",
    "    \"agg_results/qa/nortruthfulqa_mc_nn.tsv\", sep=\"\\t\", index=False\n",
    ")\n",
    "# print_latex_df(nortruthfulqa_mc_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03a75a5d-0406-45c9-b807-c24806097968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def build_ranks(df):\n",
    "    ranks = {}\n",
    "    for i, row in df.iterrows():\n",
    "        rank = np.mean([row[c] for c in dict(row) if \"Rank\" in c])\n",
    "        ranks[row[\"Model\"]] = round(rank, 3)\n",
    "    inverse = {}\n",
    "    counter = 0\n",
    "    for model, rank in sorted(ranks.items(), key=itemgetter(1)):\n",
    "        counter += 1\n",
    "        if rank not in inverse:\n",
    "            inverse[rank] = counter\n",
    "        else:\n",
    "            continue\n",
    "    return {model: inverse[rank] for model, rank in ranks.items()}\n",
    "\n",
    "\n",
    "task = \"nortruthfulqa_gen_nb\"\n",
    "\n",
    "bleu_max = aggregate_by_skill(task, add_baselines=[], target_metric=\"bleu_max\")\n",
    "bleu_max[\"Rank_bleu_max\"] = bleu_max.index.tolist()\n",
    "rougeL_max = aggregate_by_skill(task, add_baselines=[], target_metric=\"rougeL_max\")\n",
    "rougeL_max[\"Rank_rougeL_max\"] = rougeL_max.index.tolist()\n",
    "\n",
    "trthflqa_rank = build_ranks(bleu_max.merge(rougeL_max))\n",
    "trhtflqa = []\n",
    "cols = []\n",
    "\n",
    "for i, row in bleu_max.merge(rougeL_max).iterrows():\n",
    "\n",
    "    model_res = []\n",
    "\n",
    "    for k, v in dict(row).items():\n",
    "        k = (\n",
    "            k.replace(\"_max)\", \"\")\n",
    "            .replace(\"nortruthfulqa_gen_nb (\", \"\")\n",
    "            .replace(\"delta (\", \"delta \")\n",
    "        )\n",
    "        if \"Rank\" in k:\n",
    "            continue\n",
    "        if k not in cols:\n",
    "            cols.append(k)\n",
    "        model_res.append(v)\n",
    "    trhtflqa.append(model_res)\n",
    "\n",
    "df = pd.DataFrame(trhtflqa, columns=cols)\n",
    "df[\"Rank\"] = df[\"Model\"].apply(lambda x: trthflqa_rank[x])\n",
    "df = df.set_index(\"Rank\")\n",
    "df.to_csv(\"agg_results/generation/nortruthfulqa_gen_nb.tsv\", sep=\"\\t\", index=False)\n",
    "# print_latex_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92361295-0f98-4017-b9f1-e94b9892a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"mimir_bias\"\n",
    "mimir_bias = aggregate_by_skill(task, add_baselines=[], target_metric=\"pct_stereotype\")\n",
    "mimir_bias.to_csv(\"agg_results/ranking/mimir_bias.tsv\", sep=\"\\t\", index=False)\n",
    "# print_latex_df(mimir_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ac74395-59f4-4314-8674-0d1046fc10b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>bleu</th>\n",
       "      <th>delta bleu</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>delta rougeL</th>\n",
       "      <th>bertscore_f1</th>\n",
       "      <th>delta bertscore_f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@extended</td>\n",
       "      <td>22.31</td>\n",
       "      <td>xmark</td>\n",
       "      <td>42.48</td>\n",
       "      <td>xmark</td>\n",
       "      <td>73.50</td>\n",
       "      <td>xmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@extendedscratch</td>\n",
       "      <td>26.02</td>\n",
       "      <td>xmark</td>\n",
       "      <td>47.39</td>\n",
       "      <td>xmark</td>\n",
       "      <td>73.76</td>\n",
       "      <td>xmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@base</td>\n",
       "      <td>19.92</td>\n",
       "      <td>xmark</td>\n",
       "      <td>39.78</td>\n",
       "      <td>xmark</td>\n",
       "      <td>72.38</td>\n",
       "      <td>xmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@basescratch</td>\n",
       "      <td>19.23</td>\n",
       "      <td>xmark</td>\n",
       "      <td>37.62</td>\n",
       "      <td>xmark</td>\n",
       "      <td>71.40</td>\n",
       "      <td>xmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@fiction</td>\n",
       "      <td>14.19</td>\n",
       "      <td>--5.0</td>\n",
       "      <td>30.86</td>\n",
       "      <td>--6.8</td>\n",
       "      <td>65.95</td>\n",
       "      <td>--5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@nonfiction</td>\n",
       "      <td>15.15</td>\n",
       "      <td>--4.1</td>\n",
       "      <td>32.63</td>\n",
       "      <td>--5.0</td>\n",
       "      <td>65.88</td>\n",
       "      <td>--5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@factual</td>\n",
       "      <td>15.22</td>\n",
       "      <td>--4.0</td>\n",
       "      <td>32.81</td>\n",
       "      <td>--4.8</td>\n",
       "      <td>66.93</td>\n",
       "      <td>--4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@newspapers</td>\n",
       "      <td>12.52</td>\n",
       "      <td>--6.7</td>\n",
       "      <td>29.84</td>\n",
       "      <td>--7.8</td>\n",
       "      <td>68.84</td>\n",
       "      <td>--2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@books</td>\n",
       "      <td>14.75</td>\n",
       "      <td>--4.5</td>\n",
       "      <td>31.87</td>\n",
       "      <td>--5.7</td>\n",
       "      <td>66.90</td>\n",
       "      <td>--4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@rightholders</td>\n",
       "      <td>17.86</td>\n",
       "      <td>--1.4</td>\n",
       "      <td>36.13</td>\n",
       "      <td>--1.5</td>\n",
       "      <td>68.57</td>\n",
       "      <td>--2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@untranslatedwithnewspapers</td>\n",
       "      <td>15.79</td>\n",
       "      <td>--3.4</td>\n",
       "      <td>34.34</td>\n",
       "      <td>--3.3</td>\n",
       "      <td>68.34</td>\n",
       "      <td>--3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@untranslated</td>\n",
       "      <td>14.35</td>\n",
       "      <td>--4.9</td>\n",
       "      <td>31.55</td>\n",
       "      <td>--6.1</td>\n",
       "      <td>65.77</td>\n",
       "      <td>--5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@translated</td>\n",
       "      <td>13.26</td>\n",
       "      <td>--6.0</td>\n",
       "      <td>29.90</td>\n",
       "      <td>--7.7</td>\n",
       "      <td>65.64</td>\n",
       "      <td>--5.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model   bleu delta bleu  rougeL delta rougeL  \\\n",
       "Rank                                                                       \n",
       "2                       @extended  22.31      xmark   42.48        xmark   \n",
       "1                @extendedscratch  26.02      xmark   47.39        xmark   \n",
       "3                           @base  19.92      xmark   39.78        xmark   \n",
       "4                    @basescratch  19.23      xmark   37.62        xmark   \n",
       "11                       @fiction  14.19      --5.0   30.86        --6.8   \n",
       "8                     @nonfiction  15.15      --4.1   32.63        --5.0   \n",
       "7                        @factual  15.22      --4.0   32.81        --4.8   \n",
       "10                    @newspapers  12.52      --6.7   29.84        --7.8   \n",
       "8                          @books  14.75      --4.5   31.87        --5.7   \n",
       "5                   @rightholders  17.86      --1.4   36.13        --1.5   \n",
       "6     @untranslatedwithnewspapers  15.79      --3.4   34.34        --3.3   \n",
       "11                  @untranslated  14.35      --4.9   31.55        --6.1   \n",
       "13                    @translated  13.26      --6.0   29.90        --7.7   \n",
       "\n",
       "      bertscore_f1 delta bertscore_f1  \n",
       "Rank                                   \n",
       "2            73.50              xmark  \n",
       "1            73.76              xmark  \n",
       "3            72.38              xmark  \n",
       "4            71.40              xmark  \n",
       "11           65.95              --5.5  \n",
       "8            65.88              --5.5  \n",
       "7            66.93              --4.5  \n",
       "10           68.84              --2.6  \n",
       "8            66.90              --4.5  \n",
       "5            68.57              --2.8  \n",
       "6            68.34              --3.1  \n",
       "11           65.77              --5.6  \n",
       "13           65.64              --5.8  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = \"norsumm_nb\"\n",
    "\n",
    "bertscore_f1_max = aggregate_by_skill(\n",
    "    task, add_baselines=[], target_metric=\"bertscore_f1_max\"\n",
    ")\n",
    "bertscore_f1_max[\"Rank_bertscore_f1_max\"] = bertscore_f1_max.index.tolist()\n",
    "bleu_max = aggregate_by_skill(task, add_baselines=[], target_metric=\"bleu_max\")\n",
    "bleu_max[\"Rank_bleu_max\"] = bleu_max.index.tolist()\n",
    "rougeL_max = aggregate_by_skill(task, add_baselines=[], target_metric=\"rougeL_max\")\n",
    "rougeL_max[\"Rank_rougeL_max\"] = rougeL_max.index.tolist()\n",
    "\n",
    "norsumm_nb_rank = build_ranks(bleu_max.merge(rougeL_max).merge(bertscore_f1_max))\n",
    "norsumm_nb = []\n",
    "cols = []\n",
    "\n",
    "for i, row in bleu_max.merge(rougeL_max).merge(bertscore_f1_max).iterrows():\n",
    "    model_res = []\n",
    "    for k, v in dict(row).items():\n",
    "        k = (\n",
    "            k.replace(\"_max)\", \"\")\n",
    "            .replace(\"norsumm_nb (\", \"\")\n",
    "            .replace(\"delta (\", \"delta \")\n",
    "        )\n",
    "        if \"Rank\" in k:\n",
    "            continue\n",
    "        if k not in cols:\n",
    "            cols.append(k)\n",
    "        model_res.append(v)\n",
    "    norsumm_nb.append(model_res)\n",
    "\n",
    "df = pd.DataFrame(norsumm_nb, columns=cols)\n",
    "df[\"Rank\"] = df[\"Model\"].apply(lambda x: norsumm_nb_rank[x])\n",
    "df = df.set_index(\"Rank\")\n",
    "df\n",
    "# df.to_csv(\"agg_results/generation/norsumm_nb.tsv\", sep=\"\\t\", index=False)\n",
    "# print_latex_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e77bb4ec-7569-4911-8266-f45017c873be",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"norsumm_nn\"\n",
    "\n",
    "bertscore_f1_max = aggregate_by_skill(\n",
    "    task, add_baselines=[], target_metric=\"bertscore_f1_max\"\n",
    ")\n",
    "bertscore_f1_max[\"Rank_bertscore_f1_max\"] = bertscore_f1_max.index.tolist()\n",
    "bleu_max = aggregate_by_skill(task, add_baselines=[], target_metric=\"bleu_max\")\n",
    "bleu_max[\"Rank_bleu_max\"] = bleu_max.index.tolist()\n",
    "rougeL_max = aggregate_by_skill(task, add_baselines=[], target_metric=\"rougeL_max\")\n",
    "rougeL_max[\"Rank_rougeL_max\"] = rougeL_max.index.tolist()\n",
    "\n",
    "norsumm_nb_rank = build_ranks(bleu_max.merge(rougeL_max).merge(bertscore_f1_max))\n",
    "norsumm_nb = []\n",
    "cols = []\n",
    "\n",
    "for i, row in bleu_max.merge(rougeL_max).merge(bertscore_f1_max).iterrows():\n",
    "    model_res = []\n",
    "    for k, v in dict(row).items():\n",
    "        k = (\n",
    "            k.replace(\"_max)\", \"\")\n",
    "            .replace(\"norsumm_nn (\", \"\")\n",
    "            .replace(\"delta (\", \"delta \")\n",
    "        )\n",
    "        if \"Rank\" in k:\n",
    "            continue\n",
    "        if k not in cols:\n",
    "            cols.append(k)\n",
    "        model_res.append(v)\n",
    "    norsumm_nb.append(model_res)\n",
    "\n",
    "df = pd.DataFrame(norsumm_nb, columns=cols)\n",
    "df[\"Rank\"] = df[\"Model\"].apply(lambda x: norsumm_nb_rank[x])\n",
    "df = df.set_index(\"Rank\")\n",
    "df.to_csv(\"agg_results/generation/norsumm_nn.tsv\", sep=\"\\t\", index=False)\n",
    "# print_latex_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d660e-f86f-4f15-b69e-f07f884234ad",
   "metadata": {},
   "source": [
    "### Multi-shot tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42dbdcac-01c5-460c-b3c7-5d1ef332e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"norec_document_nb\"\n",
    "\n",
    "add_baselines = [[\"Random\", 48.43], [\"Constant\", 40.12]]\n",
    "\n",
    "ks = [0, 1, 4]\n",
    "\n",
    "norec_document_nb = {\n",
    "    k: {task: collect_task_prompt_results(task, k=k, verbose=False)} for k in ks\n",
    "}\n",
    "\n",
    "norec_document_nb_df = pd.concat(\n",
    "    [\n",
    "        aggregate_by_skill(\n",
    "            task, add_baselines=add_baselines, overall=norec_document_nb[k], add_k=k\n",
    "        )\n",
    "        for k in ks\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "662ff5ab-b5c6-41f6-bf6e-0116af7c7c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "def reogranize(\n",
    "    df, pretty_order=[pretty_model(m) for m in canonical_order], add_baselines=[]\n",
    "):\n",
    "    res_df = df.copy()\n",
    "    res = []\n",
    "    if add_baselines:\n",
    "        pretty_order.extend(add_baselines)\n",
    "    for model in pretty_order:\n",
    "        subset = res_df[res_df[\"Model\"] == model]\n",
    "        if model in [\"Random\", \"Constant\"]:\n",
    "            subset[\"k\"] = \"xmark\"\n",
    "            res.append(pd.DataFrame([subset.iloc[0]]))\n",
    "        else:\n",
    "            res.append(subset)\n",
    "    return pd.concat(res)\n",
    "\n",
    "\n",
    "def reogranize_by_k(\n",
    "    df,\n",
    "    change_rank=True,\n",
    "    change_cols=False,\n",
    "    pretty_order=[pretty_model(m) for m in canonical_order],\n",
    "    add_baselines=[],\n",
    "):\n",
    "    res = []\n",
    "    if add_baselines:\n",
    "        pretty_order.extend(add_baselines)\n",
    "    for i, subset in df.groupby(\"k\"):\n",
    "        if change_rank:\n",
    "            subset[\"Rank\"] = subset.index.tolist()\n",
    "        for model in pretty_order:\n",
    "            k_subset = subset[subset[\"Model\"] == model]\n",
    "            if model != \"@factual\":\n",
    "                k_subset[\"k\"] = \"\"\n",
    "            res.append(k_subset)\n",
    "    res_df = pd.concat(res)  # .drop_duplicates(subset=[\"k\", \"Model\", \"Rank\"])\n",
    "    res_df = res_df.set_index(\"k\")  # .drop_duplicates()\n",
    "    res_cols = res_df.columns.tolist()\n",
    "    if change_cols and res_cols[0] != \"Rank\":\n",
    "        new_columns = deque(res_cols)\n",
    "        new_columns.rotate(1)\n",
    "        return res_df[list(new_columns)]\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "faf85205-9044-413d-af78-dad426b18a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reogranize_by_k(norec_document_nb_df, add_baselines=[\"Random\", \"Constant\"]).to_csv(\n",
    "    \"agg_results/clf/norec_document_nb.tsv\", sep=\"\\t\", index=False\n",
    ")\n",
    "# print_latex_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cbb4cdfd-7cb8-4ae8-b395-864fc50077a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"norec_sentence_nb\"\n",
    "\n",
    "add_baselines = [[\"Random\", 48.52], [\"Constant\", 40.75]]\n",
    "\n",
    "ks = [0, 1, 4, 16]\n",
    "\n",
    "norec_sentence_nb = {\n",
    "    k: {task: collect_task_prompt_results(task, k=k, verbose=False)} for k in ks\n",
    "}\n",
    "\n",
    "norec_sentence_nb_df = pd.concat(\n",
    "    [\n",
    "        aggregate_by_skill(\n",
    "            task, add_baselines=add_baselines, overall=norec_sentence_nb[k], add_k=k\n",
    "        )\n",
    "        for k in ks\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3e138fb6-be30-42d3-8b5e-79bab3faeb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = reogranize_by_k(\n",
    "    norec_sentence_nb_df[norec_sentence_nb_df[\"k\"].isin([0, 1])],\n",
    "    add_baselines=[\"Random\", \"Constant\"],\n",
    ")\n",
    "p2 = reogranize_by_k(\n",
    "    norec_sentence_nb_df[norec_sentence_nb_df[\"k\"].isin([4, 16])],\n",
    "    add_baselines=[\"Random\", \"Constant\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6c608773-6cfb-40e5-a10c-c5281ddbf2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([p1, p2]).to_csv(\n",
    "    \"agg_results/clf/norec_sentence_nb.tsv\", sep=\"\\t\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1d2ba7a7-8cf7-4f6e-b4f4-cdb72f6c0134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_latex_df(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "43cba66f-3aad-442b-8fcc-abb4d1855303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_latex_df(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c1a82ffb-7357-41a1-8284-33835d88e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"norquad_nb\"\n",
    "\n",
    "ks = [0, 1]\n",
    "\n",
    "norquad_nb = {\n",
    "    k: {task: collect_task_prompt_results(task, k=k, verbose=False)} for k in ks\n",
    "}\n",
    "\n",
    "norquad_em_df = pd.concat(\n",
    "    [\n",
    "        aggregate_by_skill(\n",
    "            task,\n",
    "            add_baselines=[],\n",
    "            target_metric=\"exact_match\",\n",
    "            overall=norquad_nb[k],\n",
    "            add_k=k,\n",
    "        )\n",
    "        for k in ks\n",
    "    ]\n",
    ")\n",
    "\n",
    "norquad_f1_df = pd.concat(\n",
    "    [\n",
    "        aggregate_by_skill(\n",
    "            task, add_baselines=[], target_metric=\"f1\", overall=norquad_nb[k], add_k=k\n",
    "        )\n",
    "        for k in ks\n",
    "    ]\n",
    ")\n",
    "\n",
    "norquad_em_df[\"Rank\"] = norquad_em_df.index.tolist()\n",
    "\n",
    "norquad_nb_df = (\n",
    "    norquad_em_df[\n",
    "        [\"Rank\", \"Model\", \"k\", \"norquad_nb (exact_match)\", \"delta (exact_match)\"]\n",
    "    ]\n",
    "    .merge(norquad_f1_df, on=[\"Model\", \"k\"])\n",
    "    .set_index(\"Rank\")\n",
    ")\n",
    "\n",
    "# print_latex_df(reogranize_by_k(norquad_nb_df, add_baselines=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2c209e0e-3d8f-4321-aa4d-460f290f9a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "reogranize_by_k(norquad_nb_df, add_baselines=[]).to_csv(\n",
    "    \"agg_results/qa/norquad_nb.tsv\", sep=\"\\t\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ca529323-b2e2-49d4-9ae0-6171a341efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"noropenbookqa_nb\"\n",
    "add_baselines = [\n",
    "    [\"Random\", 25.00],\n",
    "]\n",
    "\n",
    "ks = [0, 1, 4, 16]\n",
    "\n",
    "noropenbookqa_nb = {\n",
    "    k: {task: collect_task_prompt_results(task, k=k, verbose=False)} for k in ks\n",
    "}\n",
    "\n",
    "noropenbookqa_nb_df = pd.concat(\n",
    "    [\n",
    "        aggregate_by_skill(\n",
    "            task,\n",
    "            add_baselines=add_baselines,\n",
    "            overall=noropenbookqa_nb[k],\n",
    "            add_k=k,\n",
    "            target_metric=\"acc\",\n",
    "        )\n",
    "        for k in ks\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b19ef015-a333-4da7-b6ba-7fef9de42632",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = reogranize_by_k(\n",
    "    noropenbookqa_nb_df[noropenbookqa_nb_df[\"k\"].isin([0, 1])],\n",
    "    add_baselines=[\"Random\", \"Constant\"],\n",
    ")\n",
    "# print_latex_df(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0662d4c8-83e5-43d8-9569-c9a5e7858edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = reogranize_by_k(\n",
    "    noropenbookqa_nb_df[noropenbookqa_nb_df[\"k\"].isin([4, 16])],\n",
    "    add_baselines=[\"Random\", \"Constant\"],\n",
    ")\n",
    "# print_latex_df(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fe47daa9-9d3e-468c-a8e9-da286f375e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([p1, p2]).to_csv(\"agg_results/qa/noropenbookqa_nb.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5b2c668c-9ba4-4fc0-85b2-cc63c2b5b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"noropenbookqa_nn\"\n",
    "add_baselines = [\n",
    "    [\"Random\", 25.00],\n",
    "]\n",
    "\n",
    "ks = [0, 1, 4, 16]\n",
    "\n",
    "noropenbookqa_nn = {\n",
    "    k: {task: collect_task_prompt_results(task, k=k, verbose=False)} for k in ks\n",
    "}\n",
    "\n",
    "noropenbookqa_nn_df = pd.concat(\n",
    "    [\n",
    "        aggregate_by_skill(\n",
    "            task,\n",
    "            add_baselines=add_baselines,\n",
    "            overall=noropenbookqa_nn[k],\n",
    "            add_k=k,\n",
    "            target_metric=\"acc\",\n",
    "        )\n",
    "        for k in ks\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7911ca06-6c07-41bc-90af-601747fa7295",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = reogranize_by_k(\n",
    "    noropenbookqa_nn_df[noropenbookqa_nn_df[\"k\"].isin([0, 1])],\n",
    "    add_baselines=[\"Random\", \"Constant\"],\n",
    ")\n",
    "# print_latex_df(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c32facec-d6ae-44f9-813b-bad14773ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = reogranize_by_k(\n",
    "    noropenbookqa_nn_df[noropenbookqa_nn_df[\"k\"].isin([4, 16])],\n",
    "    add_baselines=[\"Random\", \"Constant\"],\n",
    ")\n",
    "# print_latex_df(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "951c8383-087b-4771-a54c-8499d9a40180",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([p1, p2]).to_csv(\"agg_results/qa/noropenbookqa_nn.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fc9fad00-f4a7-41b2-9144-7b60cea6b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"noropenbookqa_nb_use_fact\"\n",
    "add_baselines = [\n",
    "    [\"Random\", 25.00],\n",
    "]\n",
    "\n",
    "ks = [0, 1, 4, 16]\n",
    "\n",
    "noropenbookqa_nb_use_fact = {\n",
    "    k: {task: collect_task_prompt_results(task, k=k, verbose=False)} for k in ks\n",
    "}\n",
    "\n",
    "noropenbookqa_nb_use_fact_df = pd.concat(\n",
    "    [\n",
    "        aggregate_by_skill(\n",
    "            task,\n",
    "            add_baselines=add_baselines,\n",
    "            overall=noropenbookqa_nb_use_fact[k],\n",
    "            add_k=k,\n",
    "            target_metric=\"acc\",\n",
    "        )\n",
    "        for k in ks\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5a7d48b6-7c50-481c-82ed-9d6c9651c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e47a26ef-d0c6-4252-8d14-cd16ec7d269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = reogranize_by_k(\n",
    "    noropenbookqa_nb_use_fact_df[noropenbookqa_nb_use_fact_df[\"k\"].isin([0, 4])],\n",
    "    add_baselines=[\"Random\"],\n",
    ")\n",
    "# print_latex_df(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "061101b5-7ca2-4a90-825e-a64b1802c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = reogranize_by_k(\n",
    "    noropenbookqa_nb_use_fact_df[noropenbookqa_nb_use_fact_df[\"k\"].isin([1, 16])],\n",
    "    add_baselines=[\"Random\"],\n",
    ")\n",
    "# print_latex_df(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fc6f0bf7-608f-44d6-aaca-432ba78d9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([p1, p2]).to_csv(\n",
    "    \"agg_results/qa/noropenbookqa_nb_use_fact.tsv\", sep=\"\\t\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3a28535b-cf13-451b-a0a3-3cfe5d9417f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"noropenbookqa_nn_use_fact\"\n",
    "add_baselines = [\n",
    "    [\"Random\", 25.00],\n",
    "]\n",
    "\n",
    "ks = [0, 1, 4, 16]\n",
    "\n",
    "noropenbookqa_nn_use_fact = {\n",
    "    k: {task: collect_task_prompt_results(task, k=k, verbose=False)} for k in ks\n",
    "}\n",
    "\n",
    "noropenbookqa_nn_use_fact_df = pd.concat(\n",
    "    [\n",
    "        aggregate_by_skill(\n",
    "            task,\n",
    "            add_baselines=add_baselines,\n",
    "            overall=noropenbookqa_nn_use_fact[k],\n",
    "            add_k=k,\n",
    "            target_metric=\"acc\",\n",
    "        )\n",
    "        for k in ks\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "89953092-911f-4c98-ba3f-b1037a870375",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = reogranize_by_k(\n",
    "    noropenbookqa_nn_use_fact_df[noropenbookqa_nn_use_fact_df[\"k\"].isin([0, 4])],\n",
    "    add_baselines=[\"Random\"],\n",
    ")\n",
    "# print_latex_df(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1b5e2875-0a07-4021-800d-471502143af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = reogranize_by_k(\n",
    "    noropenbookqa_nn_use_fact_df[noropenbookqa_nn_use_fact_df[\"k\"].isin([1, 16])],\n",
    "    add_baselines=[\"Random\"],\n",
    ")\n",
    "# print_latex_df(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "da0a8bae-d0da-450e-94d2-98e2b015b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([p1, p2]).to_csv(\n",
    "    \"agg_results/qa/noropenbookqa_nn_use_fact.tsv\", sep=\"\\t\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0dd74ed9-2c68-498d-bc61-87573f15f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 260\n",
    "task = \"ask_gec_nb\"\n",
    "ask_gec = pd.read_csv(\"ask_gec_nb/ask_gec_k_shot.tsv\", sep=\"\\t\").rename(\n",
    "    columns={\"k\": \"k-shot\"}\n",
    ")\n",
    "ask_gec[\"task\"] = task\n",
    "\n",
    "ask_gec = ask_gec[[\"task\", \"model\", \"prompt\", \"k-shot\", \"errant\"]]\n",
    "\n",
    "ask_gec_overall = {k: {task: subset} for k, subset in ask_gec.groupby(\"k-shot\")}\n",
    "\n",
    "ask_gec_df = pd.concat(\n",
    "    [\n",
    "        aggregate_by_skill(\n",
    "            task,\n",
    "            add_baselines=[],\n",
    "            overall=ask_gec_overall[k],\n",
    "            add_k=k,\n",
    "            target_metric=\"errant\",\n",
    "        )\n",
    "        for k in ask_gec_overall\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9d766ee2-169d-4eac-bad3-84e7b3337fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = reogranize_by_k(\n",
    "    ask_gec_df[ask_gec_df[\"k\"].isin([0, 1])], add_baselines=[], change_cols=True\n",
    ")\n",
    "# print_latex_df(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "af786826-c6f8-48fc-89a5-40731d1eb4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = reogranize_by_k(\n",
    "    ask_gec_df[ask_gec_df[\"k\"].isin([1, 16])], add_baselines=[], change_cols=True\n",
    ")\n",
    "# print_latex_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bfdf3979-bac4-494a-9caa-fcff7429a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([p1, p2]).to_csv(\n",
    "    \"agg_results/generation/ask_gec_nb.tsv\", sep=\"\\t\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "de7c8fdc-e000-4506-b82b-0ee6683a96c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "task2metric_bs = {\n",
    "    \"tatoeba_eng_nno_nn\": [\"bertscore_f1\"],\n",
    "    \"tatoeba_nno_eng_nn\": [\"bertscore_f1\"],\n",
    "    \"tatoeba_eng_nob_nb\": [\"bertscore_f1\"],\n",
    "    \"tatoeba_nob_eng_nb\": [\"bertscore_f1\"],\n",
    "    \"tatoeba_nob_nno_nb\": [\"bertscore_f1\"],\n",
    "    \"tatoeba_nno_nob_nn\": [\"bertscore_f1\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d72aa9aa-7114-4da7-9b13-9a84c9d40392",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore_mt = pd.read_csv(\"bertscore_mt_k_shot.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8a75839b-9bfe-4152-bd08-0d2892dfd871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>bertscore_f1</th>\n",
       "      <th>k</th>\n",
       "      <th>prompt</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mimir-mistral-7b-base-scratch</td>\n",
       "      <td>86.471</td>\n",
       "      <td>0</td>\n",
       "      <td>prompt_0</td>\n",
       "      <td>tatoeba_eng_nno_nn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mimir-mistral-7b-base-scratch</td>\n",
       "      <td>79.985</td>\n",
       "      <td>0</td>\n",
       "      <td>prompt_2</td>\n",
       "      <td>tatoeba_eng_nno_nn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model  bertscore_f1  k    prompt  \\\n",
       "0  mimir-mistral-7b-base-scratch        86.471  0  prompt_0   \n",
       "1  mimir-mistral-7b-base-scratch        79.985  0  prompt_2   \n",
       "\n",
       "                 task  \n",
       "0  tatoeba_eng_nno_nn  \n",
       "1  tatoeba_eng_nno_nn  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertscore_mt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9f6b2d96-4788-4044-ae30-355bc73216ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mt_results(\n",
    "    task,\n",
    "    ks,\n",
    "    bertscore_mt=bertscore_mt,\n",
    "    task2metric_bs=task2metric_bs,\n",
    "    change_cols=True,\n",
    "    task2metric=task2metric,\n",
    "):\n",
    "    mt = {k: {task: collect_task_prompt_results(task, k=k, verbose=False)} for k in ks}\n",
    "\n",
    "    mt_bs = {\n",
    "        k: {task: subset}\n",
    "        for k, subset in bertscore_mt[bertscore_mt[\"task\"] == task].groupby(\"k\")\n",
    "    }\n",
    "\n",
    "    mt_res = {}\n",
    "\n",
    "    target_cols = [\n",
    "        \"Rank\",\n",
    "        \"Model\",\n",
    "        \"bleu\",\n",
    "        \"delta bleu\",\n",
    "        \"chrf\",\n",
    "        \"delta chrf\",\n",
    "        \"bertscore_f1\",\n",
    "        \"delta bertscore_f1\",\n",
    "        \"k\",\n",
    "    ]\n",
    "\n",
    "    for k in ks:\n",
    "        bleu = aggregate_by_skill(\n",
    "            task,\n",
    "            add_baselines=[],\n",
    "            target_metric=\"bleu\",\n",
    "            overall=mt[k],\n",
    "            add_k=k,\n",
    "            task2metric=task2metric,\n",
    "        )\n",
    "        bleu[\"Rank_bleu\"] = bleu.index.tolist()\n",
    "\n",
    "        chrf = aggregate_by_skill(\n",
    "            task,\n",
    "            add_baselines=[],\n",
    "            target_metric=\"chrf\",\n",
    "            overall=mt[k],\n",
    "            add_k=k,\n",
    "            task2metric=task2metric,\n",
    "        )\n",
    "        chrf[\"Rank_chrf\"] = chrf.index.tolist()\n",
    "\n",
    "        bertscore = aggregate_by_skill(\n",
    "            task,\n",
    "            add_baselines=[],\n",
    "            target_metric=\"bertscore_f1\",\n",
    "            overall=mt_bs[k],\n",
    "            add_k=k,\n",
    "            task2metric=task2metric_bs,\n",
    "        )\n",
    "        bertscore[\"Rank_bertscore\"] = bertscore.index.tolist()\n",
    "\n",
    "        merged = bleu.merge(chrf).merge(bertscore)\n",
    "        ranks = build_ranks(merged)\n",
    "        k_res, cols = [], []\n",
    "        for i, row in merged.iterrows():\n",
    "            model_res = []\n",
    "            for key, v in dict(row).items():\n",
    "                key = (\n",
    "                    key.replace(\"_max)\", \"\")\n",
    "                    .replace(f\"{task} (\", \"\")\n",
    "                    .replace(\"delta (\", \"delta \")\n",
    "                    .rstrip(\")\")\n",
    "                )\n",
    "                if \"Rank\" in key:\n",
    "                    continue\n",
    "                if key not in cols:\n",
    "                    cols.append(key)\n",
    "                model_res.append(v)\n",
    "            k_res.append(model_res)\n",
    "        k_df = pd.DataFrame(k_res, columns=cols)\n",
    "        k_df[\"Rank\"] = k_df[\"Model\"].apply(lambda x: ranks[x])\n",
    "        mt_res[k] = k_df[target_cols]\n",
    "    return mt_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "71a488ae-d05e-49cf-89e8-f279cae4f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [0, 1, 4, 16]\n",
    "\n",
    "tatoeba_eng_nob_nb = get_mt_results(task=\"tatoeba_eng_nob_nb\", ks=ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "839e7c49-0948-4af6-9310-b328b580b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = reogranize_by_k(tatoeba_eng_nob_nb[0], change_rank=False, add_baselines=[])\n",
    "p2 = reogranize_by_k(tatoeba_eng_nob_nb[4], change_rank=False, add_baselines=[])\n",
    "p3 = reogranize_by_k(tatoeba_eng_nob_nb[1], change_rank=False, add_baselines=[])\n",
    "p4 = reogranize_by_k(tatoeba_eng_nob_nb[16], change_rank=False, add_baselines=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "10fdcf1e-9b30-439d-aed9-68e04b27f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([p1, p3, p2, p4]).to_csv(\n",
    "    \"agg_results/generation/tatoeba_eng_nob_nb.tsv\", sep=\"\\t\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5fc21daf-140a-4793-8a2d-ba4f42610ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tatoeba_nob_eng_nb = get_mt_results(task=\"tatoeba_nob_eng_nb\", ks=ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "45c4c24a-71b9-4f33-a985-a8a847eb76d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = reogranize_by_k(tatoeba_nob_eng_nb[0], change_rank=False, add_baselines=[])\n",
    "p2 = reogranize_by_k(tatoeba_nob_eng_nb[1], change_rank=False, add_baselines=[])\n",
    "p3 = reogranize_by_k(tatoeba_nob_eng_nb[4], change_rank=False, add_baselines=[])\n",
    "p4 = reogranize_by_k(tatoeba_nob_eng_nb[16], change_rank=False, add_baselines=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8defdf4a-25ad-4912-a3d7-d31f55c62cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([p1, p2, p3, p4]).to_csv(\n",
    "    \"agg_results/generation/tatoeba_nob_eng_nb.tsv\", sep=\"\\t\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c5ca5f34-2f3f-4872-af26-f2d556dda31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_latex_df(p1)\n",
    "# print_latex_df(p2)\n",
    "# print_latex_df(p3)\n",
    "# print_latex_df(p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "bc0f7238-a75a-489a-947e-849b861d887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tatoeba_eng_nno_nn = get_mt_results(task=\"tatoeba_eng_nno_nn\", ks=ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d4e0a9ff-e24f-4784-ba7e-6d3164b111f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = reogranize_by_k(tatoeba_eng_nno_nn[0], change_rank=False, add_baselines=[])\n",
    "p2 = reogranize_by_k(tatoeba_eng_nno_nn[1], change_rank=False, add_baselines=[])\n",
    "p3 = reogranize_by_k(tatoeba_eng_nno_nn[4], change_rank=False, add_baselines=[])\n",
    "p4 = reogranize_by_k(tatoeba_eng_nno_nn[16], change_rank=False, add_baselines=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1276d15f-a9d2-49e9-b59f-56ad64a8571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([p1, p2, p3, p4]).to_csv(\n",
    "    \"agg_results/generation/tatoeba_eng_nno_nn.tsv\", sep=\"\\t\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "42b0256d-701c-4354-8e14-5f944b8b26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_latex_df(p1)\n",
    "# print_latex_df(p2)\n",
    "# print_latex_df(p3)\n",
    "# print_latex_df(p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "209ae479-50be-4ca0-b94c-aa8a3a30863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tatoeba_nno_eng_nn = get_mt_results(task=\"tatoeba_nno_eng_nn\", ks=ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9e0722c2-655f-401f-8af7-86c54b596b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = reogranize_by_k(tatoeba_nno_eng_nn[0], change_rank=False, add_baselines=[])\n",
    "p2 = reogranize_by_k(tatoeba_nno_eng_nn[1], change_rank=False, add_baselines=[])\n",
    "p3 = reogranize_by_k(tatoeba_nno_eng_nn[4], change_rank=False, add_baselines=[])\n",
    "p4 = reogranize_by_k(tatoeba_nno_eng_nn[16], change_rank=False, add_baselines=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b6719022-5e28-4ca7-8d99-98f69eada66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([p1, p2, p3, p4]).to_csv(\n",
    "    \"agg_results/generation/tatoeba_nno_eng_nn.tsv\", sep=\"\\t\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "f59dc331-581e-4929-adf2-dd018e9e1b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_latex_df(p1)\n",
    "# print_latex_df(p2)\n",
    "# print_latex_df(p3)\n",
    "# print_latex_df(p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e8f3f1e3-fddb-4f93-83c2-f1f5e385aafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tatoeba_nob_nno_nb = get_mt_results(task=\"tatoeba_nob_nno_nb\", ks=[0])\n",
    "del tatoeba_nob_nno_nb[0][\"k\"]\n",
    "p1 = reogranize(tatoeba_nob_nno_nb[0].set_index(\"Rank\"), add_baselines=[])\n",
    "p1.to_csv(\"agg_results/generation/tatoeba_nob_nno_nb.tsv\", sep=\"\\t\", index=False)\n",
    "# print_latex_df(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "db8e46ba-0b25-4ddd-b722-5a783d1e6ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/pfs/lustrep3/scratch/project_465000498/vlad/mimir/mimir_results/agg_results.zip'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(\"agg_results\", \"zip\", \"agg_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "47cb1add-5551-48a6-94dd-796e6b482cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrlrlrl}\n",
      "\\toprule\n",
      "{} &                        Model &   bleu & delta bleu &   chrf & delta chrf &  bertscore\\_f1 & delta bertscore\\_f1 \\\\\n",
      "Rank &                              &        &            &        &            &               &                    \\\\\n",
      "\\midrule\n",
      "7    &                    \\extended &  10.49 &      \\xmark &  42.58 &      \\xmark &         87.49 &              \\xmark \\\\\n",
      "5    &             \\extendedscratch &  12.50 &      \\xmark &  47.18 &      \\xmark &         88.00 &              \\xmark \\\\\n",
      "9    &                        \\base &  10.96 &      \\xmark &  43.29 &      \\xmark &         84.74 &              \\xmark \\\\\n",
      "2    &                 \\basescratch &  24.36 &      \\xmark &  64.14 &      \\xmark &         94.53 &              \\xmark \\\\\n",
      "8    &                     \\fiction &   8.93 &     --15.4 &  38.74 &     --25.4 &         89.16 &              --5.4 \\\\\n",
      "10   &                  \\nonfiction &   9.79 &     --14.6 &  40.60 &     --23.5 &         85.84 &              --8.7 \\\\\n",
      "3    &                     \\factual &  17.06 &      --7.3 &  55.19 &      --9.0 &         93.80 &              --0.7 \\\\\n",
      "1    &                  \\newspapers &  45.31 &      +21.0 &  77.56 &      +13.4 &         96.08 &               +1.5 \\\\\n",
      "13   &                       \\books &   8.48 &     --15.9 &  37.37 &     --26.8 &         85.78 &              --8.8 \\\\\n",
      "5    &                \\rightholders &  11.80 &     --12.6 &  45.52 &     --18.6 &         91.70 &              --2.8 \\\\\n",
      "4    &  \\untranslatedwithnewspapers &  16.53 &      --7.8 &  53.50 &     --10.6 &         93.38 &              --1.2 \\\\\n",
      "11   &                \\untranslated &   8.77 &     --15.6 &  37.97 &     --26.2 &         86.22 &              --8.3 \\\\\n",
      "12   &                  \\translated &   8.72 &     --15.6 &  37.42 &     --26.7 &         85.27 &              --9.3 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tatoeba_nno_nob_nn = get_mt_results(task=\"tatoeba_nno_nob_nn\", ks=[0])\n",
    "del tatoeba_nno_nob_nn[0][\"k\"]\n",
    "p1 = reogranize(tatoeba_nno_nob_nn[0].set_index(\"Rank\"), add_baselines=[])\n",
    "p1.to_csv(\"agg_results/generation/tatoeba_nno_nob_nn.tsv\", sep=\"\\t\", index=False)\n",
    "print_latex_df(p1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
